{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bab2249",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b86e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9092bd9d",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b18c0f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://github.com/soumik12345/point-cloud-segmentation/releases/download/v0.1/shapenet.zip\"\n",
    "\n",
    "dataset_path = tf.keras.utils.get_file(\n",
    "    fname=\"shapenet.zip\",\n",
    "    origin=dataset_url,\n",
    "    cache_subdir=\"datasets\",\n",
    "    hash_algorithm=\"auto\",\n",
    "    extract=True,\n",
    "    archive_format=\"auto\",\n",
    "    cache_dir=\"datasets\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0597c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Airplane': {'directory': '02691156',\n",
       "  'lables': ['wing', 'body', 'tail', 'engine'],\n",
       "  'colors': ['blue', 'green', 'red', 'pink']},\n",
       " 'Bag': {'directory': '02773838',\n",
       "  'lables': ['handle', 'body'],\n",
       "  'colors': ['blue', 'green']},\n",
       " 'Cap': {'directory': '02954340',\n",
       "  'lables': ['panels', 'peak'],\n",
       "  'colors': ['blue', 'green']},\n",
       " 'Car': {'directory': '02958343',\n",
       "  'lables': ['wheel', 'hood', 'roof'],\n",
       "  'colors': ['blue', 'green', 'red']},\n",
       " 'Chair': {'directory': '03001627',\n",
       "  'lables': ['leg', 'arm', 'back', 'seat'],\n",
       "  'colors': ['blue', 'green', 'red', 'pink']},\n",
       " 'Earphone': {'directory': '03261776',\n",
       "  'lables': ['earphone', 'headband'],\n",
       "  'colors': ['blue', 'green']},\n",
       " 'Guitar': {'directory': '03467517',\n",
       "  'lables': ['head', 'body', 'neck'],\n",
       "  'colors': ['blue', 'green', 'red']},\n",
       " 'Knife': {'directory': '03624134',\n",
       "  'lables': ['handle', 'blade'],\n",
       "  'colors': ['blue', 'green']},\n",
       " 'Lamp': {'directory': '03636649',\n",
       "  'lables': ['canopy', 'lampshade', 'base'],\n",
       "  'colors': ['blue', 'green', 'red']},\n",
       " 'Laptop': {'directory': '03642806',\n",
       "  'lables': ['keyboard'],\n",
       "  'colors': ['blue']},\n",
       " 'Motorbike': {'directory': '03790512',\n",
       "  'lables': ['wheel', 'handle', 'gas_tank', 'light', 'seat'],\n",
       "  'colors': ['blue', 'green', 'red', 'pink', 'yellow']},\n",
       " 'Mug': {'directory': '03797390', 'lables': ['handle'], 'colors': ['blue']},\n",
       " 'Pistol': {'directory': '03948459',\n",
       "  'lables': ['trigger_and_guard', 'handle', 'barrel'],\n",
       "  'colors': ['blue', 'green', 'red']},\n",
       " 'Rocket': {'directory': '04099429',\n",
       "  'lables': ['nose', 'body', 'fin'],\n",
       "  'colors': ['blue', 'green', 'red']},\n",
       " 'Skateboard': {'directory': '04225987',\n",
       "  'lables': ['wheel', 'deck'],\n",
       "  'colors': ['blue', 'green']},\n",
       " 'Table': {'directory': '04379243',\n",
       "  'lables': ['leg', 'top'],\n",
       "  'colors': ['blue', 'green']}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/tmp/.keras/datasets/PartAnnotation/metadata.json\") as json_file:\n",
    "    metadata = json.load(json_file)\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbd65b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_dir = \"/tmp/.keras/datasets/PartAnnotation/{}/points\".format(\n",
    "    metadata[\"Airplane\"][\"directory\"]\n",
    ")\n",
    "labels_dir = \"/tmp/.keras/datasets/PartAnnotation/{}/points_label\".format(\n",
    "    metadata[\"Airplane\"][\"directory\"]\n",
    ")\n",
    "LABELS = metadata[\"Airplane\"][\"lables\"]\n",
    "COLORS = metadata[\"Airplane\"][\"colors\"]\n",
    "VAL_SPLIT = 0.2\n",
    "N_SAMPLE_POINTS = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ab3757",
   "metadata": {},
   "source": [
    "## Filtering the files that have labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be00ff5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 4045/4045 [00:00<00:00, 16213.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3694"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_files_with_keys = set()\n",
    "points_files = glob(os.path.join(points_dir, \"*.pts\"))\n",
    "\n",
    "\n",
    "for point_file in tqdm(points_files):\n",
    "    file_id = point_file.split(\"/\")[-1].split(\".\")[0]\n",
    "    label_data = {}\n",
    "\n",
    "    for label in LABELS:\n",
    "        label_file = os.path.join(labels_dir, label, file_id + \".seg\")\n",
    "        if os.path.exists(label_file):\n",
    "            label_data[label] = 0  # Dummy assignment only used as a placeholder.\n",
    "    try:\n",
    "        label_data = np.vstack(tuple([label_data[key] for key in LABELS]))\n",
    "        points_files_with_keys.add(point_file)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "points_files_with_keys = list(points_files_with_keys)\n",
    "len(points_files_with_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36edb4cf",
   "metadata": {},
   "source": [
    "## Prepare dataset utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df1ff3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_point_file(point_filepath: str):\n",
    "    # Load the point cloud from disk.\n",
    "    point_filepath = point_filepath.numpy().decode(\"utf-8\")\n",
    "    point_cloud = np.loadtxt(point_filepath)\n",
    "\n",
    "    # Parse the file-id.\n",
    "    file_id = point_filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "    label_data, num_labels = {}, 0\n",
    "\n",
    "    # Parse the labels.\n",
    "    for label in LABELS:\n",
    "        label_file = os.path.join(labels_dir, label, file_id + \".seg\")\n",
    "        label_data[label] = np.loadtxt(label_file).astype(\"float32\")\n",
    "        num_labels = len(label_data[label])\n",
    "\n",
    "    label_map = [\"none\"] * num_labels\n",
    "    for label in LABELS:\n",
    "        for i, data in enumerate(label_data[label]):\n",
    "            label_map[i] = label if data == 1 else label_map[i]\n",
    "    label_data = np.vstack(tuple([label_data[key] for key in LABELS]))\n",
    "    label_cloud = label_data.reshape(label_data.shape[1], label_data.shape[0])\n",
    "\n",
    "    # Sample `N_SAMPLE_POINTS` from the point and label clouds randomly.\n",
    "    sampled_point_cloud, sampled_label_cloud = random_sampler(point_cloud, label_cloud)\n",
    "\n",
    "    # Normalizing point cloud.\n",
    "    normalized_point_cloud = sampled_point_cloud - np.mean(sampled_point_cloud, axis=0)\n",
    "    normalized_point_cloud /= np.max(np.linalg.norm(normalized_point_cloud, axis=1))\n",
    "\n",
    "    return normalized_point_cloud, sampled_label_cloud\n",
    "\n",
    "\n",
    "def random_sampler(point_cloud: np.ndarray, label_cloud: np.ndarray):\n",
    "    n_points = len(point_cloud)\n",
    "\n",
    "    # Randomly sampling respective indices.\n",
    "    sampled_indices = random.sample(list(range(n_points)), N_SAMPLE_POINTS)\n",
    "\n",
    "    # Sampling points corresponding to sampled indices.\n",
    "    sampled_point_cloud = np.array([point_cloud[i] for i in sampled_indices])\n",
    "\n",
    "    # Sampling corresponding one-hot encoded labels.\n",
    "    sampled_label_cloud = np.array([label_cloud[i] for i in sampled_indices])\n",
    "\n",
    "    return sampled_point_cloud, sampled_label_cloud\n",
    "\n",
    "\n",
    "def tf_process_point_file(point_filepath: str):\n",
    "    data_tuple = tf.py_function(\n",
    "        process_single_point_file, [point_filepath], [tf.float64, tf.float32]\n",
    "    )\n",
    "    return data_tuple\n",
    "\n",
    "\n",
    "def augment(point_cloud_batch, label_cloud_batch):\n",
    "    # Jitter point and label clouds.\n",
    "    noise = tf.random.uniform(\n",
    "        tf.shape(label_cloud_batch), -0.005, 0.005, dtype=tf.float64\n",
    "    )\n",
    "    point_cloud_batch += noise[:, :, :3]\n",
    "    label_cloud_batch += tf.cast(noise, tf.float32)\n",
    "\n",
    "    return point_cloud_batch, label_cloud_batch\n",
    "\n",
    "\n",
    "def prepare_dataset(\n",
    "    point_filepaths: List[str], is_train: bool = True, batch_size: int = 16\n",
    "):\n",
    "    point_files_ds = tf.data.Dataset.from_tensor_slices(point_filepaths)\n",
    "    if is_train:\n",
    "        point_files_ds = point_files_ds.shuffle(batch_size * 100)\n",
    "\n",
    "    point_ds = point_files_ds.map(\n",
    "        tf_process_point_file, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    point_ds = point_ds.batch(batch_size)\n",
    "    if is_train:\n",
    "        point_ds = point_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    return point_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598e32d",
   "metadata": {},
   "source": [
    "## Create `tf.data.Dataset` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9788337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-05 16:39:02.238126: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'/tmp/.keras/datasets/PartAnnotation/02691156/points/ecc50d702133b1531e9da99095f71c63.pts'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_files_ds = tf.data.Dataset.from_tensor_slices(points_files_with_keys)\n",
    "single_pcloud_file = next(iter(point_files_ds))\n",
    "single_pcloud_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfc28a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1024, 3), (1024, 4))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_pcloud_file = next(iter(point_files_ds))\n",
    "point_cloud, label_cloud = process_single_point_file(single_pcloud_file)\n",
    "point_cloud.shape, label_cloud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc90cd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-05 16:39:02.384411: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 3) (1024, 4)\n",
      "(1024, 3) (1024, 4)\n",
      "(1024, 3) (1024, 4)\n",
      "(1024, 3) (1024, 4)\n",
      "(1024, 3) (1024, 4)\n"
     ]
    }
   ],
   "source": [
    "point_ds = point_files_ds.map(\n",
    "    tf_process_point_file, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "for point_cloud, label_cloud in point_ds.take(5):\n",
    "    print(point_cloud.shape, label_cloud.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94967499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training files: 2955.\n",
      "Total validation files: 739.\n"
     ]
    }
   ],
   "source": [
    "split_index = int(len(points_files_with_keys) * (1 - VAL_SPLIT))\n",
    "train_point_cloud_files = points_files_with_keys[:split_index]\n",
    "val_point_cloud_files = points_files_with_keys[split_index:]\n",
    "\n",
    "print(f\"Total training files: {len(train_point_cloud_files)}.\")\n",
    "print(f\"Total validation files: {len(val_point_cloud_files)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b7b5fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = prepare_dataset(train_point_cloud_files)\n",
    "validation_ds = prepare_dataset(val_point_cloud_files, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc42af5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=<unknown>, dtype=tf.float64, name=None),\n",
       "  TensorSpec(shape=<unknown>, dtype=tf.float32, name=None)),\n",
       " (TensorSpec(shape=<unknown>, dtype=tf.float64, name=None),\n",
       "  TensorSpec(shape=<unknown>, dtype=tf.float32, name=None)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec, validation_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3eb11c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([16, 1024, 3]), TensorShape([16, 1024, 4]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for single_batch in train_ds.take(1):\n",
    "    break\n",
    "    \n",
    "single_batch[0].shape, single_batch[1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
