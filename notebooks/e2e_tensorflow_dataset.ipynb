{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3b8713c",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/soumik12345/point-cloud-segmentation/blob/dataloader-fix/notebooks/e2e_tensorflow_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab2249",
   "metadata": {
    "id": "5bab2249"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b86e390",
   "metadata": {
    "id": "9b86e390"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9092bd9d",
   "metadata": {
    "id": "9092bd9d"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c0f0d",
   "metadata": {
    "id": "b18c0f0d"
   },
   "outputs": [],
   "source": [
    "dataset_url = \"https://github.com/soumik12345/point-cloud-segmentation/releases/download/v0.1/shapenet.zip\"\n",
    "\n",
    "dataset_path = tf.keras.utils.get_file(\n",
    "    fname=\"shapenet.zip\",\n",
    "    origin=dataset_url,\n",
    "    cache_subdir=\"datasets\",\n",
    "    hash_algorithm=\"auto\",\n",
    "    extract=True,\n",
    "    archive_format=\"auto\",\n",
    "    cache_dir=\"datasets\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0597c59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0597c59",
    "outputId": "74e1d764-29f7-40ec-b980-5c7025451851"
   },
   "outputs": [],
   "source": [
    "with open(\"/tmp/.keras/datasets/PartAnnotation/metadata.json\") as json_file:\n",
    "    metadata = json.load(json_file)\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd65b00",
   "metadata": {
    "id": "dbd65b00"
   },
   "outputs": [],
   "source": [
    "points_dir = \"/tmp/.keras/datasets/PartAnnotation/{}/points\".format(\n",
    "    metadata[\"Airplane\"][\"directory\"]\n",
    ")\n",
    "labels_dir = \"/tmp/.keras/datasets/PartAnnotation/{}/points_label\".format(\n",
    "    metadata[\"Airplane\"][\"directory\"]\n",
    ")\n",
    "LABELS = metadata[\"Airplane\"][\"lables\"]\n",
    "COLORS = metadata[\"Airplane\"][\"colors\"]\n",
    "VAL_SPLIT = 0.2\n",
    "N_SAMPLE_POINTS = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ab3757",
   "metadata": {
    "id": "a3ab3757"
   },
   "source": [
    "## Filtering the files that have labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00ff5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be00ff5a",
    "outputId": "bd3e2698-2d41-4000-dd3f-9ee68291734c"
   },
   "outputs": [],
   "source": [
    "points_files_with_keys = set()\n",
    "points_files = glob(os.path.join(points_dir, \"*.pts\"))\n",
    "\n",
    "\n",
    "for point_file in tqdm(points_files):\n",
    "    file_id = point_file.split(\"/\")[-1].split(\".\")[0]\n",
    "    label_data = {}\n",
    "\n",
    "    for label in LABELS:\n",
    "        label_file = os.path.join(labels_dir, label, file_id + \".seg\")\n",
    "        if os.path.exists(label_file):\n",
    "            label_data[label] = 0  # Dummy assignment only used as a placeholder.\n",
    "    try:\n",
    "        label_data = np.vstack(tuple([label_data[key] for key in LABELS]))\n",
    "        points_files_with_keys.add(point_file)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "points_files_with_keys = list(points_files_with_keys)\n",
    "len(points_files_with_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36edb4cf",
   "metadata": {
    "id": "36edb4cf"
   },
   "source": [
    "## Prepare dataset utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ff3c7",
   "metadata": {
    "id": "df1ff3c7"
   },
   "outputs": [],
   "source": [
    "def process_single_point_file(point_filepath: str):\n",
    "    # Load the point cloud from disk.\n",
    "    point_filepath = point_filepath.numpy().decode(\"utf-8\")\n",
    "    point_cloud = np.loadtxt(point_filepath)\n",
    "\n",
    "    # Parse the file-id.\n",
    "    file_id = point_filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "    label_data, num_labels = {}, 0\n",
    "\n",
    "    # Parse the labels.\n",
    "    for label in LABELS:\n",
    "        label_file = os.path.join(labels_dir, label, file_id + \".seg\")\n",
    "        label_data[label] = np.loadtxt(label_file).astype(\"float32\")\n",
    "        num_labels = len(label_data[label])\n",
    "\n",
    "    label_map = [\"none\"] * num_labels\n",
    "    for label in LABELS:\n",
    "        for i, data in enumerate(label_data[label]):\n",
    "            label_map[i] = label if data == 1 else label_map[i]\n",
    "    label_data = [\n",
    "        LABELS.index(label) if label != \"none\" else len(LABELS)\n",
    "        for label in label_map\n",
    "    ]\n",
    "    label_cloud = tf.keras.utils.to_categorical(\n",
    "        label_data, num_classes=len(LABELS) + 1\n",
    "    )\n",
    "\n",
    "    # Sample `N_SAMPLE_POINTS` from the point and label clouds randomly.\n",
    "    sampled_point_cloud, sampled_label_cloud = random_sampler(point_cloud, label_cloud)\n",
    "\n",
    "    # Normalizing point cloud.\n",
    "    normalized_point_cloud = sampled_point_cloud - np.mean(sampled_point_cloud, axis=0)\n",
    "    normalized_point_cloud /= np.max(np.linalg.norm(normalized_point_cloud, axis=1))\n",
    "\n",
    "    return normalized_point_cloud, sampled_label_cloud\n",
    "\n",
    "\n",
    "def random_sampler(point_cloud: np.ndarray, label_cloud: np.ndarray):\n",
    "    n_points = len(point_cloud)\n",
    "\n",
    "    # Randomly sampling respective indices.\n",
    "    sampled_indices = random.sample(list(range(n_points)), N_SAMPLE_POINTS)\n",
    "\n",
    "    # Sampling points corresponding to sampled indices.\n",
    "    sampled_point_cloud = np.array([point_cloud[i] for i in sampled_indices])\n",
    "\n",
    "    # Sampling corresponding one-hot encoded labels.\n",
    "    sampled_label_cloud = np.array([label_cloud[i] for i in sampled_indices])\n",
    "\n",
    "    return sampled_point_cloud, sampled_label_cloud\n",
    "\n",
    "\n",
    "def tf_process_point_file(point_filepath: str):\n",
    "    data_tuple = tf.py_function(\n",
    "        process_single_point_file, [point_filepath], [tf.float64, tf.float32]\n",
    "    )\n",
    "    return data_tuple\n",
    "\n",
    "\n",
    "def augment(point_cloud_batch, label_cloud_batch):\n",
    "    # Jitter point and label clouds.\n",
    "    noise = tf.random.uniform(\n",
    "        tf.shape(label_cloud_batch), -0.005, 0.005, dtype=tf.float64\n",
    "    )\n",
    "    point_cloud_batch += noise[:, :, :3]\n",
    "    label_cloud_batch += tf.cast(noise, tf.float32)\n",
    "\n",
    "    return point_cloud_batch, label_cloud_batch\n",
    "\n",
    "\n",
    "def prepare_dataset(\n",
    "    point_filepaths: List[str], is_train: bool = True, batch_size: int = 16\n",
    "):\n",
    "    point_files_ds = tf.data.Dataset.from_tensor_slices(point_filepaths)\n",
    "    if is_train:\n",
    "        point_files_ds = point_files_ds.shuffle(batch_size * 100)\n",
    "\n",
    "    point_ds = point_files_ds.map(\n",
    "        tf_process_point_file, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    point_ds = point_ds.batch(batch_size)\n",
    "    if is_train:\n",
    "        point_ds = point_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    return point_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598e32d",
   "metadata": {
    "id": "4598e32d"
   },
   "source": [
    "## Create `tf.data.Dataset` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9788337",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9788337",
    "outputId": "651431e5-3653-47cd-cdb5-b710b0c3fdd8"
   },
   "outputs": [],
   "source": [
    "point_files_ds = tf.data.Dataset.from_tensor_slices(points_files_with_keys)\n",
    "single_pcloud_file = next(iter(point_files_ds))\n",
    "single_pcloud_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc28a3f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfc28a3f",
    "outputId": "6f0f589e-6eae-48b7-bbad-c5a5e9235e87"
   },
   "outputs": [],
   "source": [
    "single_pcloud_file = next(iter(point_files_ds))\n",
    "point_cloud, label_cloud = process_single_point_file(single_pcloud_file)\n",
    "point_cloud.shape, label_cloud.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aWBExxfLCnW4",
   "metadata": {
    "id": "aWBExxfLCnW4"
   },
   "source": [
    "### Visualizing sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0YrOfuqFBo63",
   "metadata": {
    "id": "0YrOfuqFBo63"
   },
   "outputs": [],
   "source": [
    "def visualize_data_plotly(point_cloud, labels):\n",
    "    fig = px.scatter_3d(\n",
    "        pd.DataFrame(\n",
    "            data={\n",
    "                'x': point_cloud[:, 0],\n",
    "                'y': point_cloud[:, 1],\n",
    "                'z': point_cloud[:, 2],\n",
    "                'label': labels\n",
    "            }\n",
    "        ), x=\"x\", y=\"y\", z=\"z\",\n",
    "        color=\"label\", labels={\"label\": \"Label\"},\n",
    "        color_discrete_sequence=COLORS + ['black'],\n",
    "        category_orders={\"label\": LABELS + ['none']}\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s_uttqzdDLXG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "s_uttqzdDLXG",
    "outputId": "75691357-8021-4c2f-92c4-3d941eb7594e"
   },
   "outputs": [],
   "source": [
    "label_map = LABELS + ['none']\n",
    "visualize_data_plotly(point_cloud, [label_map[np.argmax(label)] for label in label_cloud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gZmTkihFCcfF",
   "metadata": {
    "id": "gZmTkihFCcfF"
   },
   "outputs": [],
   "source": [
    "def visualize_data_plt(point_cloud, labels):\n",
    "    color_map = COLORS + ['black']\n",
    "    df = pd.DataFrame(\n",
    "        data={\n",
    "            'x': point_cloud[:, 0],\n",
    "            'y': point_cloud[:, 1],\n",
    "            'z': point_cloud[:, 2],\n",
    "            'label': labels,\n",
    "        }\n",
    "    )\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    ax = plt.axes(projection='3d')  \n",
    "    for index, label in enumerate(LABELS + ['none']):\n",
    "        c_df = df[df['label'] == label]\n",
    "        try:\n",
    "            ax.scatter(\n",
    "                c_df['x'], c_df['y'], c_df['z'],\n",
    "                label=label, alpha = 0.5, c=color_map[index]\n",
    "            ) \n",
    "        except IndexError:\n",
    "            pass\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N9yhaMQnCe5m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "N9yhaMQnCe5m",
    "outputId": "125c54cf-2e23-4d0d-831d-71cf8796bdcf"
   },
   "outputs": [],
   "source": [
    "visualize_data_plt(point_cloud, [label_map[np.argmax(label)] for label in label_cloud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc90cd37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc90cd37",
    "outputId": "c16a7595-ddec-4d3a-a2bc-4f9b36af61fb"
   },
   "outputs": [],
   "source": [
    "point_ds = point_files_ds.map(\n",
    "    tf_process_point_file, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "for point_cloud, label_cloud in point_ds.take(5):\n",
    "    print(point_cloud.shape, label_cloud.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94967499",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94967499",
    "outputId": "ccce9cab-4f8c-43e1-dffa-0929a8fff908"
   },
   "outputs": [],
   "source": [
    "split_index = int(len(points_files_with_keys) * (1 - VAL_SPLIT))\n",
    "train_point_cloud_files = points_files_with_keys[:split_index]\n",
    "val_point_cloud_files = points_files_with_keys[split_index:]\n",
    "\n",
    "print(f\"Total training files: {len(train_point_cloud_files)}.\")\n",
    "print(f\"Total validation files: {len(val_point_cloud_files)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b5fc1",
   "metadata": {
    "id": "8b7b5fc1"
   },
   "outputs": [],
   "source": [
    "train_ds = prepare_dataset(train_point_cloud_files)\n",
    "validation_ds = prepare_dataset(val_point_cloud_files, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc42af5b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc42af5b",
    "outputId": "c2b214a6-a531-48a1-d600-6c06ad28fbd4"
   },
   "outputs": [],
   "source": [
    "train_ds.element_spec, validation_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb11c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3eb11c7",
    "outputId": "0b2502bc-1df4-41ab-f4f6-7794d78d80d0"
   },
   "outputs": [],
   "source": [
    "for single_batch in train_ds.take(1):\n",
    "    break\n",
    "    \n",
    "single_batch[0].shape, single_batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RXyfQ1MbFGWz",
   "metadata": {
    "id": "RXyfQ1MbFGWz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "e2e_tensorflow_dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
